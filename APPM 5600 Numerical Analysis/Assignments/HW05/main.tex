\documentclass[a4paper,12pt]{article}

\usepackage[utf8]{inputenc}
\usepackage[left=0.5in,right=0.5in,top=1in,bottom=1in]{geometry}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{pgfplots,graphicx,calc,changepage}
\pgfplotsset{compat=newest}
\usepackage{enumitem}
\usepackage{fancyhdr}
\usepackage[colorlinks = true, linkcolor = blue]{hyperref}

% Syntax highlighting
\usepackage{listings}
\usepackage{xcolor}

\definecolor{codegreen}{rgb}{0.40,0.62,0.07}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codeblue}{rgb}{0.09,0.57,0.73}
\definecolor{backcolour}{rgb}{1,1,1}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codeblue},
    basicstyle=\ttfamily\small,
    breaklines=true,                     
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,
    showstringspaces=false,
    showtabs=false,                  
    tabsize=4
}

\lstset{style=mystyle}

\newcommand{\nats}{\mathbb{N}}
\newcommand{\reals}{\mathbb{R}}
\newcommand{\rats}{\mathbb{Q}}
\newcommand{\ints}{\mathbb{Z}}
\newcommand{\comps}{\mathbb{C}}
\newcommand{\pols}{\mathcal{P}}
\newcommand{\cants}{\Delta\!\!\!\!\Delta}
\newcommand{\eps}{\varepsilon}
\newcommand{\st}{\backepsilon}
\newcommand{\abs}[1]{\left| #1 \right|}
\newcommand{\dom}[1]{\mathrm{dom}\left(#1\right)}
\newcommand{\for}{\text{ for }}
\newcommand{\dd}[1]{\mathrm{d}#1}
\newcommand{\spn}{\mathrm{sp}}
\newcommand{\nul}{\mathcal{N}}
\newcommand{\col}{\mathrm{col}}
\newcommand{\rank}{\mathrm{rank}}
\newcommand{\norm}[1]{\lVert #1 \rVert}
\newcommand{\inner}[1]{\left\langle #1 \right\rangle}
\newcommand{\pmat}[1]{\begin{pmatrix} #1 \end{pmatrix}}
\renewcommand{\and}{\text{ and }}

\newsavebox{\qed}
\newenvironment{proof}[2][$\square$]
    {\setlength{\parskip}{0pt}\par\textit{Proof:} #2\setlength{\parskip}{0.25cm}
        \savebox{\qed}{#1}
        \begin{adjustwidth}{\widthof{Proof:}}{}
    }
    {
        \hfill\usebox{\qed}\end{adjustwidth}
    }

\pagestyle{fancy}
\fancyhead{}
\lhead{Caleb Jacobs}
\chead{APPM 5600: Numerical Analysis I}
\rhead{Homework \#5}
\cfoot{}
\setlength{\headheight}{35pt}
\setlength{\parskip}{0.25cm}
\setlength{\parindent}{0pt}

\begin{document}
\section*{Problems}
\begin{enumerate}[label = \arabic*.]
	\item Let $ A \in \reals^{n \times n} $ be a tridiagonal matrix where the diagonal entries are given by $ a_j $ for $ j = 1, \ldots, n $, the lower diagonal entries are $ b_j $ for $ j = 2, \ldots, n $ and the upper diagonal entries are $ c_j $ for $ j = 1, \ldots, n-1 $.
	\begin{enumerate}[label = (\alph*)]
		\item For $ n = 3 $, derive the $ LU $ factorization of the matrix A.
		\[
			\begin{array}{ccccc}
				U = \pmat{a_1 & c_1 & 0 \\ b_1 & a_2 & c_2 \\ 0 & b_2 & a_3} & \to & \pmat{a_1 & c_1 & 0 \\ 0 & a_2 - \frac{c_1 b_1}{a_1} & c_2 \\ 0 & b_2 & a_3} & \to & \pmat{a_1 & c_1 & 0 \\ 0 & a_2 - \frac{c_1 b_1}{a_1} & c_2 \\ 0 & 0 & a_3 - \frac{c_2 b_2}{a_2 - \frac{c_1 b_1}{a_1}}} \\
				L = \pmat{1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1} & \to & \pmat{1 & 0 & 0 \\ \frac{b_1}{a_1} & 1 & 0 \\ 0 & 0 & 1} & \to & \pmat{1 & 0 & 0 \\ \frac{b_1}{a_1} & 1 & 0 \\ 0 & \frac{b_2}{a_2 - \frac{c_1 b_1}{a_1}} & 1}.
			\end{array}
		\]
		So, our LU factorization in $ n = 3 $ is given by 
		\[
			L = \pmat{1 & 0 & 0 \\ \frac{b_1}{a_1} & 1 & 0 \\ 0 & \frac{b_2}{a_2 - \frac{c_1 b_1}{a_1}} & 1}
		\]
		and
		\[
			U = \pmat{a_1 & c_1 & 0 \\ 0 & a_2 - \frac{c_1 b_1}{a_1} & c_2 \\ 0 & 0 & a_3 - \frac{c_2 b_2}{a_2 - \frac{c_1 b_1}{a_1}}}.
		\]
		
		\item What is the extension of the LU factorization for general $ n $?
		
		Looking at the $ n = 3 $ case, we can see that the next entry in $ U $ and $ L $ can be turned into an iterative process. The iteration process is as follows. 
		\begin{enumerate}[label = (\arabic*)]
			\item Set $ U $ to be the zero $ n \times n $ matrix and set $ L $ to be the $ n \times n $ identity matrix.
			\item Set $ U_{11} = a_1$.
			\item Set $ k = 1 $.
			\item Set $ U_{k+1, k+1} = a_k - \frac{c_k b_k}{U_{k,k}} $.
			\item Set $ U_{k, k+1} = c_k $.
			\item Set $ L_{k+1, k} = \frac{b_k}{U_{k,k}} $
			\item Increase $ k $ by 1 and then repeat at step (4) until done.
		\end{enumerate}
	
		\item What is the operation count when applying Gaussian Elimination to a tridiagonal system without pivoting.
		
		Looking at our operation count in part (b), we can see that step 1, 2, and 3, take 0 flops. Next, step 4 takes 3 flops. Because we are just doing Gaussian Elimination and we don't need to form $ LU $, we can skip the rest of the steps except for the repeat step which occurs $ n - 1 $ times. Thus, the total cost is given by 
		\[
			3 (n - 1) = 3n - 3 \; \text{flops}.
		\]
	\end{enumerate}

	\item Consider the linear system
	\begin{align*}
		6x + 2y + 2z &= -2 \\
		2x + \frac{2}{3}y + \frac{1}{3} z &= 1 \\
		x + 2y - z &= 0
	\end{align*}

	\begin{enumerate}[label = (\alph*)]
		\item Verify that $ (x,y,z) = (2.6, -3.8, -5) $ is the exact solution.
			
			To verify the solution, let's first rewrite the LHS of the system and multiply by our vector to get
			\[
				\pmat{
					6 & 2 & 2 \\
					2 & \frac{2}{3} & \frac{1}{3} \\
					1 & 2 & -1
				}\pmat{
					2.6 \\
					-3.8 \\
					-5
				} = \pmat{
					-2 \\
					1 \\
					0
				}
			\]
			which shows the exact solution is given by $ (x,y,z) = (2.6, -3.8, -5) $.
			
			\item Let's create our augmented matrix and begin Gaussian elimination
			\begin{align*}
				\left(\begin{array}{c c c | c}
					6 & 2 & 2 & -2 \\
					2 & \frac{2}{3} & \frac{1}{3} & 1\\
					1 & 2 & -1 & 0
				\end{array}\right) 
				&\sim 
				\left(\begin{array}{c c c | c}
					1 & 0.3333 & 0.3333 & -0.3333 \\
					2 & 0.6667 & 0.3333 & 1 \\
					1 & 2 & -1 & 0
				\end{array}\right) \\
				&\sim
				\left(\begin{array}{c c c | c}
					1 & 0.3333 & 0.3333 & -2 \\
					0 & 0.0001 & -0.3333 & 1.666 \\
					0 & 1.666 & -1.333 & 0.3333
				\end{array}\right) \\ 
				&\sim
				\left(\begin{array}{c c c | c}
					1 & 0.3333 & 0.3333 & -2 \\
					0 & 1 & -3333 & 16660 \\
					0 & 1.666 & -1.333 & 0.3333
				\end{array}\right) \\ 
				&\sim
				\left(\begin{array}{c c c | c}
					1 & 0 & 1111 & -5554 \\
					0 & 1 & -3333 & 16660 \\
					0 & 0 & 5551 & -27740
				\end{array}\right) \\ 
				&\sim
				\left(\begin{array}{c c c | c}
					1 & 0 & 1111 & -5554 \\
					0 & 1 & -3333 & 16660 \\
					0 & 0 & 1 & -4.997
				\end{array}\right) \\ 
				&\sim
				\left(\begin{array}{c c c | c}
					1 & 0 & 0 & -3 \\
					0 & 1 & 0 & 10 \\
					0 & 0 & 1 & -4.997
				\end{array}\right). 
			\end{align*}
			So, our solution in 4 digit arithmetic without pivoting is given by $ (x,y,z) = (-3, 10, -4.997) $ which has an absolute error of 14.893
						
			\item Repeat part (b) with partial pivoting.
			\begin{align*}
				\left(\begin{array}{c c c | c}
					6 & 2 & 2 & -2 \\
					2 & \frac{2}{3} & \frac{1}{3} & 1\\
					1 & 2 & -1 & 0
				\end{array}\right) 
				&\sim 
				\left(\begin{array}{c c c | c}
					6 & 2 & 2 & -2 \\
					2 & 0.6667 & 0.3333 & 1 \\
					1 & 2 & -1 & 0
				\end{array}\right) \\
				&\sim
				\left(\begin{array}{c c c | c}
					6 & 2 & 2 & -2 \\
					0 & 0.0001 & 0.3333 & 1 \\
					0 & 1.666 & -1.333 & 0.3333
				\end{array}\right) \\
				&\sim
				\left(\begin{array}{c c c | c}
					6 & 2 & 2 & -2 \\
					0 & 1.666 & -1.333 & 0.3333 \\
					0 & 0.0001 & 0.3333 & 1
				\end{array}\right) \\
			&\sim
			\left(\begin{array}{c c c | c}
				6 & 2 & 2 & -2 \\
				0 & 1.666 & -1.333 & 0.3333 \\
				0 & 0 & 0.3333 & 1
			\end{array}\right)
			\end{align*}
			which implies $ z = 3.000 $, $ y = 2.6 $, and $ x = -2.644 $ which has an absolute error of 11.5091.
			
			\item Gaussian elimination with partial pivoting was slightly more accurate in this case and kept us from losing so many significant digits by reducing divisions by relatively small numbers.
	\end{enumerate}

	\item Consider the system $ Ax = b $ where
	\[
		A = \pmat{
			4 & -1 & 0 &-1 & 0 & 0 \\
			-1 & 4 & -1 & 0 & -1 & 0 \\
			0 & -1 & 4 & -1 & 0 & -1 \\
			-1 & 0 & -1 & 4 & -1& 0 \\
			0 & -1 & 0 & -1 & 4 & -1 \\
			0 & 0 & -1 & 0 & -1 & 4
		} \quad b = \pmat{
			2 \\ 1\\2\\2\\1\\2
		}.
	\]
	
	\emph{All code used can be found at the end of the document}
	With $ x_0 = [1 1 1 1 1 1]^T $,
	\begin{enumerate}[label = (\alph*)]
		\item use Gauss-Jacobi iteration to approximate the solution to this problem $ \varepsilon = 1e-7 $.
		
		Using Gauss-Jacobi iteration, my code converged in 37 iterations to a solution of
		\[
			x = \pmat{
				1.166666550360919 \\
				1.208333174456728 \\
				1.458333174456728 \\
				1.458333174456728 \\
				1.208333174456728 \\
				1.166666550360919
			}.
		\]
		
		\item use Gauss-Siedel iteration to approximate the solution to this problem $ \varepsilon = 1e-7 $.
		
		Using Gauss-Siedel iteration, my code converged in 20 iterations to a solution of
		\[
		x = \pmat{
			1.166666582106617 \\
			1.208333241597320 \\
			1.458333255911479 \\
			1.458333262516811 \\
			1.208333275046199 \\
			1.166666632739420
		}.
		\]
		
		\item use SOR iteration with $ \omega = 1.6735 $ to approximate the solution to this problem $ \varepsilon = 1e-7 $.
		
		Using SOR iteration, my code converged in 47 iterations to a solution of
		\[
		x = \pmat{
			1.166666642205604 \\
			1.208333349651239 \\
			1.458333301323380 \\
			1.458333326774206 \\
			1.208333348402415 \\
			1.166666649988570
			}.
		\]
		
		\item For this exact problem, Gauss-Siedel had the fastest convergence. In general, this will not be true, especially if we pick a nice $ \omega $ for SOR. Furthermore, each of these methods is very sensitive to the input matrix which means performance of each method will very considerably from matrix to matrix.
		
		\item Set $ c = \rho(B) $ (spectral radius). Use the following error estimate to derive error bounds for the last computed approximations with all methods.
		\[
			\norm{x_{k+1} - x} \leq \frac{c}{1 - c} \norm{x_{k + 1} - x_k}
		\]
		
		\begin{itemize}
			\item For Gauss-Jacobi iteration, $ B = -D^{-1}(L + U) $. Then, from my MATLAB code, the spectral radius is given of $ B $ is 
			\[
				c = \rho(B) = 0.683012701892219
			\]
			which implies an error bound of 
			\[
				\norm{x_{k + 1}} \leq \frac{c}{1 - c} \norm{x_{k+1} - x_k} = 2.1547 \norm{x_{k+1} - x_k}.
			\]
			\item For Gauss-Siedel iteration, $ B = -(L + D)^{-1} U $. Then, from my MATLAB code, the spectral radius is given of $ B $ is 
			\[
				c = \rho(B) = 0.480583134298243
			\]
			which implies an error bound of 
			\[
				\norm{x_{k + 1}} \leq \frac{c}{1 - c} \norm{x_{k+1} - x_k} = 0.925236 \norm{x_{k+1} - x_k}.
			\]
			\item For SOR iteration, $ B = -(D + \omega L)^{-1}(\omega U + (\omega - 1)D)$. Then, from my MATLAB code, the spectral radius is given of $ B $ is 
			\[
				c = \rho(B) = 0.725728486720244
			\]
			which implies an error bound of 
			\[
				\norm{x_{k + 1}} \leq \frac{c}{1 - c} \norm{x_{k+1} - x_k} = 2.64602 \norm{x_{k+1} - x_k}.
			\]
		\end{itemize}
		
		
		\item What happens if you change the parameter $ \omega $ for SOR?
		
		Changing $ \omega $ can change the convergence rate of SOR. As for the iterations, when $ \omega $ is large, the iterations can jump further which can cause an oscillatory convergence pattern. If $ \omega $ is too small, then the iterations move slower and slower but are also more predictable. For fastest convergence rates, we need some $ \omega $ that balances movement with stability.
	\end{enumerate}

	\item The linear system of equation
	\[
		\pmat{1 & -a \\ -a & 1} x = b
	\]
	where $ a \in \reals $ under certain conditions can be solved by the iterative method
	\[
		\pmat{1 & 0 \\ -\omega a & 1} x_{k + 1} = \pmat{ 1- \omega & \omega a \\ 0 & 1 - \omega}
	\]
	\begin{enumerate}[label = (\alph*)]
		\item For which values of $ a $ is the method convergent for $ \omega = 1 $?
		
		First, let's compute the spectral radius of
		\[
			\pmat{ 1& 0 \\ -a & 1}^{-1} \pmat{0 & a \\ 0 & 0} = \pmat{ 1& 0 \\ a & 1} \pmat{0 & a \\ 0 & 0} = \pmat{0 & a \\ 0 & a^2}.
		\]
		which has eigenvalues $ \lambda = 0 $ and $ \lambda = a^2 $ which implies the spectral radius is
		\[
			\sigma(B) = a^2.
		\]
		Then, for convergence, we must have $ \sigma(B) < 1 $ which implies $ a^2 < 1 $ or $ a \in (-1, 1) $ for convergence.
		
		\item For $ a = 0.5 $, find the value of $ \omega \in \{0.8, 0.9, 1.0, 1.1, 1.2, 1.3\} $ which minimizes the spectral radius of the matrix
		\[
			\pmat{1 & 0 \\ -\omega a & 1}^{-1} \pmat{1- \omega & \omega a \\ 0 & 1- \omega}.
		\]
		
		First, let's expand this matrix out to get
		\[
			\pmat{1 & 0 \\ -\omega a & 1}^{-1} \pmat{1- \omega & \omega a \\ 0 & 1- \omega} = \pmat{1 - \omega & \frac{1}{2}\omega \\ \frac{1}{2} \omega (1 - \omega) & \frac{1}{4} (w - 2)^2}.
		\]
		Then, using my MATLAB code, we can see that the spectral radius of our matrix is minimized when $ \omega = 1.1 $ which gives our matrix a spectral radius of $ 0.1 $.
	\end{enumerate}

\newpage
\section*{Code Used}
	\rule{\textwidth}{0.4pt}
	\lstinputlisting[language = matlab]{"code/Iteratives.m"}
	\rule{\textwidth}{0.4pt}
\end{enumerate}
\end{document}