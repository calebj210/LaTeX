\documentclass[a4paper,12pt]{article}

\usepackage[T1]{fontenc} \usepackage{unicode-math}
\usepackage[left=0.5in,right=0.5in,top=1in,bottom=1in]{geometry}
\usepackage{amsmath,amsfonts}
\usepackage{pgfplots,graphicx,calc,changepage}
\pgfplotsset{compat=newest}
\usepackage{enumitem}
\usepackage{fancyhdr}
\usepackage[colorlinks = true, linkcolor = black]{hyperref}
\usepackage{nameref}

% Syntax highlighting
\usepackage{listings}
\usepackage{xcolor}

\definecolor{codegreen}{rgb}{0.40,0.62,0.07}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codeblue}{rgb}{0.09,0.57,0.73}
\definecolor{backcolour}{rgb}{1,1,1}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codeblue},
    basicstyle=\ttfamily\small,
    breaklines=true,                     
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,
    showstringspaces=false,
    showtabs=false,                  
    tabsize=4
}

\lstset{style=mystyle}

\lstdefinelanguage{Julia}%
{morekeywords={abstract,break,case,catch,const,continue,do,else,elseif,%
		end,export,false,for,function,immutable,import,importall,if,in,%
		macro,module,otherwise,quote,return,switch,true,try,type,typealias,%
		using,while},%
	sensitive=true,%
	alsoother={$},%
	morecomment=[l]\#,%
	morecomment=[n]{\#=}{=\#},%
	morestring=[s]{"}{"},%
	morestring=[m]{'}{'},%
}[keywords,comments,strings]%

\newcommand{\nats}{\mathbb{N}}
\newcommand{\reals}{\mathbb{R}}
\newcommand{\rats}{\mathbb{Q}}
\newcommand{\ints}{\mathbb{Z}}
\newcommand{\comps}{\mathbb{C}}
\newcommand{\pols}{\mathcal{P}}
\newcommand{\cants}{\Delta\!\!\!\!\Delta}
\newcommand{\eps}{\varepsilon}
\newcommand{\st}{\backepsilon}
\newcommand{\abs}[1]{\left| #1 \right|}
\newcommand{\dom}[1]{\mathrm{dom}\left(#1\right)}
\newcommand{\for}{\text{ for }}
\newcommand{\dd}{\mathrm{d}}
\newcommand{\spn}{\mathrm{sp}}
\newcommand{\nul}{\mathcal{N}}
\newcommand{\col}{\mathrm{col}}
\newcommand{\rank}{\mathrm{rank}}
\newcommand{\norm}[1]{\lVert #1 \rVert}
\newcommand{\inner}[1]{\left\langle #1 \right\rangle}
\newcommand{\pmat}[1]{\begin{pmatrix} #1 \end{pmatrix}}
\renewcommand{\and}{\text{ and }}

\newsavebox{\qed}
\newenvironment{proof}[2][$\square$]
    {\setlength{\parskip}{0pt}\par\textit{Proof:} #2\setlength{\parskip}{0.25cm}
        \savebox{\qed}{#1}
        \begin{adjustwidth}{\widthof{Proof:}}{}
    }
    {
        \hfill\usebox{\qed}\end{adjustwidth}
    }

\pagestyle{fancy}
\fancyhead{}
\lhead{Caleb Jacobs}
\chead{APPM 5610: Numerical Analysis II}
\rhead{Homework \#2}
\cfoot{}
\setlength{\headheight}{35pt}
\setlength{\parskip}{0.25cm}
\setlength{\parindent}{0pt}

\begin{document}
\begin{enumerate}[label = (\arabic*)]
	\item Show that the Hilbert matrix is positive definite.
	
	\begin{proof}{}
		Suppose we have the $ n \times n $ Hilbert matrix
		\[
			H_{ij} = \frac{1}{i + j - 1}, \quad i,j = 1, \ldots, n.
		\]
		Then, to show that $ H $ is positive definite, we need to show that $ x^* H x > 0 $ for all $ x \neq 0 $ in $ \comps^n $. So, suppose we have any nonzero $ x \in \comps^n $.
		Then,
		\begin{align*}
			x^* H x = \sum_{i = 1}^{n} \left(\bar{x}_i \sum_{j = 1}^{n} x_j H_{ij}\right) &= \sum_{i = 1}^{n} \sum_{j = 1}^{n} \bar{x}_i x_j \frac{1}{i + j - 1} \\
			&= \sum_{i = 1}^{n} \sum_{j = 1}^{n} \bar{x}_i x_j \int_{0}^{1} t^{i + j - 2} \dd t \\
			&= \sum_{i = 1}^{n} \sum_{j = 1}^{n} \int_{0}^{1} (\bar{x}_i t^{i - 1}) (x_j t^{j - 1}) \dd t \\
			&= \int_{0}^{1} \sum_{i = 1}^{n} \sum_{j = 1}^{n} (\bar{x}_i t^{i - 1}) (x_j t^{j - 1}) \dd t \\
			&= \int_{0}^{1} \left(\sum_{i = 1}^{n} \bar{x}_i t^{i - 1}\right) \left(\sum_{j = 1}^{n} x_j t^{j - 1}\right) \dd t.
		\end{align*}
		Now if we let $ \alpha = \sum_{j = 1}^{n} x_j t^{j - 1} $, our equation becomes
		\begin{align*}
			x^* H x &= \int_{0}^{1} \left(\sum_{i = 1}^{n} \bar{x}_i t^{i - 1}\right) \left(\sum_{j = 1}^{n} x_j t^{j - 1}\right) \dd t \\
			&= \int_{0}^{1} \bar{\alpha} \alpha \dd t \\
			&= \int_{0}^{1} \abs{\alpha}^2 \dd t \\
			&> 0.
		\end{align*}
		So, our Hilbert matrix must be positive definite.
	\end{proof}

	\newpage
	\item Using my power iteration code (attached in section: \nameref{sec:code}), I generated the table below of the largest eigenvalues for the first 10 square Hilbert matrices.
	
	\[
		\begin{array}{rcl}
			n  & \text{Eigenvalue} & \text{Eigenvector} \\
			1  & 1.00 & [1.0]^T \\
			2  & 1.27 & [0.88, 0.47]^T \\
			3  & 1.41 & [0.83, 0.46, 0.32]^T \\
			4  & 1.50 & [0.79, 0.45, 0.32, 0.25]^T \\
			5  & 1.57 & [0.77, 0.45, 0.32, 0.25, 0.21]^T \\
			6  & 1.62 & [0.75, 0.44, 0.32, 0.25, 0.21, 0.18] \\
			7  & 1.66 & [0.73, 0.44, 0.32, 0.25, 0.21, 0.18, 0.16]^T \\
			8  & 1.70 & [0.72, 0.43, 0.32, 0.26, 0.21, 0.18, 0.16, 0.15]^T \\
			9  & 1.73 & [0.71, 0.43, 0.32, 0.26, 0.21, 0.19, 0.16, 0.15, 0.13]^T \\
			10 & 1.75 & [0.70, 0.43, 0.32, 0.26, 0.22, 0.19, 0.16, 0.15, 0.13, 0.12]^T
		\end{array}
	\]
	
	\newpage
	\item To find the smallest eigenvalues using a power iteration, I just change the matrix-vector multiplication to a backslash as to solve $ Ax = b $ instead of computing $ x = Ab $. This change has the effect of making the smallest eigenvalue of $ A $ dominate the power iteration. For $ n = 16 $, I obtained the eigenvalue:
	\[
		\lambda_{\min} = -2.765686556840889 \cdot 10^{-18}
	\]
	which is not even positive even though we know Hilbert matrices are symmetric positive definite. So my naive Julia code is definitely not converging properly under double precision. Putting my Julia code in quad precision yields the eigenvalue
	\[
		\lambda_{\min} =  9.197419820719313\cdot 10^{-23}.
	\]
	which is consistent with Mathematica's eigenvalue. Hence, we can see that my computed double-precision eigenvalue is off by over 5 orders of magnitude. However, this result is still consistent with our error estimate
	\[
		\min_{\lambda \in \sigma(A)} \abs{\mu - \lambda} \leq \norm{E}_2
	\]
	because $ \norm{E}_2 $ should be on the order of $ 10^{-16} $ which is the best we can do in double precision.
	
	\newpage
	\item Assume that a real matrix $ A $ has eigenvalues $ \lambda_1 = -\lambda_2 $ and $ \abs{\lambda_1} = \abs{\lambda_2} > \abs{\lambda_3} \geq \abs{\lambda_n} $. 
	
	Without loss of generality, assume $ \lambda_1 > 0 $ which makes $ \lambda_2 < 0 $. Now, note that because $ A $ is symmetric, eigenvectors corresponding to different eigenvalues of $ A $ are orthogonal. Then, to find the eigenvectors corresponding to $ \lambda_1 $ and $ \lambda_2 $, run the standard power iteration to find some normalized vector, $ \vec{v}_0 $, in the span of $ \vec{\lambda}_1 $ and $ \vec{\lambda}_2 $ where $ \vec{\lambda}_1 $ and $ \vec{\lambda}_2 $ are eigenvectors corresponding to $ \lambda_1 $ and $ \lambda_2 $ respectively. Then, because $ \vec{\lambda}_1 $ and $ \vec{\lambda}_2 $ are orthogonal, we can uniquely decompose $ \vec{v}_0 $ as 
	\[
		\vec{v}_0 = a \vec{\lambda}_1 + b \vec{\lambda}_2
	\]
	for some constants $ a $ and $ b $. Then, one more power iteration on $ \vec{v}_0 $ will yield
	\[
		\vec{v}_1 = a \vec{\lambda}_1 - b \vec{\lambda}_2
	\]
	because $ \lambda_2 < 0 $ and the eigenvectors are orthogonal. Then, we can simply find an eigenvector corresponding to $ \lambda_1 $ as
	\[
		\vec{v}_0 + \vec{v}_1 = 2a \vec{\lambda}_1
	\]
	and an eigenvector corresponding to $ \lambda_2 $ as 
	\[
		\vec{v}_0 - \vec{v}_1 = 2b \vec{\lambda}_2.
 	\]
 	
 	\newpage
 	\item A real symmetric matrix $ A $ has an eigenvalue 1 of multiplicity $ 8 $; the rest of the eigenvalues are $ \leq 0.1 $ in absolute value.
 	
 	We can find an orthogonal basis for the $ 8 $-dimensional eigenspace corresponding to the dominant eigenvalue by combining the standard power iteration with the Gram-Schmidt process. To do so:
 	
 	\begin{enumerate}[label = (\roman*)]
 		\item Generate 8, random vectors.
 		\item Apply power iteration to each vector until convergence. The resulting vectors will be in the span of the dominant eigenvectors.
 		\item Apply Gram-Schmidt to the set of generated vectors.
 		\item You're all done!
 	\end{enumerate}
 	
 	To estimate how long it will take to find this basis for an $ n \times n $ matrix, we need to figure out how long it will take each power iteration to converge to double-precision. If each subdominant eigenvalue has a magnitude less than or equal to $ 0.1 $, we should expect each power iteration to converge in 16 to 17 iterations because $ \abs{\lambda_{\text{next}} /
 	\lambda_1} \leq 0.1 $. Then, we need to run power iteration for each of the 8 vectors for a total of 136 iterations.
\end{enumerate}

\newpage
\section*{Code Used}\label{sec:code}
\emph{Note that some characters are missing because \LaTeX\ can't display some of the unicode characters in my code.}

\rule{\textwidth}{0.4pt}
	\lstinputlisting[language = Julia]{Code/Power_Iteration.jl}
\rule{\textwidth}{0.4pt}
\end{document}